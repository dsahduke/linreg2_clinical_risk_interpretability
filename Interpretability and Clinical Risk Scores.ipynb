{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One of the reasons that linear regression and variants are so popular is because they are *interpretable*. When you see the equation for the regression, you can immediately see how the variables are interacting and be able to think through how the model works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In many applications, this is a really important feature to have. If your model is being used a decision support tool, it is important that there is trust in the model, or else it may not get used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is interpretability?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Many machine learning models, as they get more complicated, become less interpretable. Modern neural networks can contain millions of parameters, and it is infeasible to trace the computations as they happen. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "People want interpretable models, but it is difficult to quantify what that actually means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "> The demand for interpretability arises when\n",
    "there is a mismatch between the formal objectives of supervised learning (test set predictive performance) and the real\n",
    "world costs in a deployment setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Lipton 2016 *The Mythos of Model Interpretability***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](./assets/blackbox.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Trust:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Thought experiment 1:\n",
    "    \n",
    "    A. Doctors can diagnose a particular condition with 90% accuracy and can tell you why\n",
    "    B. A black box model can diagnose it with 92% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Thought experiment 2:\n",
    "    \n",
    "    A. Doctors can diagnose a particular condition with 70% accuracy and can tell you why\n",
    "    B. A black box model can diagnose it with 72% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Thought experiment 3:\n",
    "    \n",
    "    A. Doctors can diagnose a particular condition with 70% accuracy and can tell you why\n",
    "    B. A black box model can diagnose it with 80% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If the model gets examples wrong that a human would normally get right, perhaps human supervision is warranted. Is predictive accuracy the only thing that we care about?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Transferability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A model created by Caruana et al(2015)  \n",
    " * http://people.dbmi.columbia.edu/noemie/papers/15kdd.pdf \n",
    " \n",
    "showed that for Pneumonia patients, patients that had Asthma were at *lower* risk than those without asthma. This was due to the fact that people with asthma were directly admitted to intensive care units and so received more aggressive treatment. Therefore, it matters the context with which the data was gathered. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Potential properties of interpretable models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Transparency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Informally, this is the opposite of blackbox-ness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Three ways that a model can be transparent:\n",
    "    1. *simulatability* Can a human, in a reasonable time frame, step through the entire model?\n",
    "    2. *decomposability* Do each of the features make sense? Are they intuitive? Do each of the coefficients have meaning?\n",
    "    3. *algorithmic transparency* Do we understand how the algorithms work that are used to fit the models? Linear Regression we know has a unique solution and converges, Deep neural networks we are unsure of."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Post-hoc interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Can we explain the output of a model after it has already been trained? Through text, pictures, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Lam et al 2018 [Automated Detection of Diabetic Retinopathy using Deep Learning](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5961805/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](./assets/retinal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](./assets/explanation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Chest XRAY\n",
    "https://medium.com/@jrzech/what-are-radiological-deep-learning-models-actually-learning-f97a546c5b98"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](./assets/cardiomegaly1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](./assets/placement.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](./assets/portable.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Free book on interpretable machine learning: https://christophm.github.io/interpretable-ml-book/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Clinical Risk Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Clinical risk scores are scores that are derived from patient characteristics, lab values, etc. that can be used to predict an adverse outcome in a patient. If this sounds like machine learning, it often is! However, many of these models, which are still in use today, often did not have that much data to back them up. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "https://www.mdcalc.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[Clinical Score for Predicting Recurrence After Hepatic Resection for Metastatic Colorectal Cancer](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1420876/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Published: 1999\n",
    "\n",
    "Sample Size: 1001 patients\n",
    "(metastatic colorectal cancer)\n",
    "\n",
    "1 point for each factor: \n",
    " * node-positive primary\n",
    " * disease-free interval from primary to metastases < 12 months\n",
    " * number of heptatic tumors > 1\n",
    " * largest hepatic tumor > 35cm\n",
    " * carcinoembryonic antigen level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](./assets/metastatic_crc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[$CHADS_2$](https://www.ncbi.nlm.nih.gov/pubmed/11401607) Risk for Atrial Fibrillation Stroke"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Published: 2001\n",
    "\n",
    "Sample: 1733 patients, aged 65 to 95 years\n",
    "\n",
    "2 points for:\n",
    " * history of stroke or TIA\n",
    "\n",
    "1 point for:\n",
    " * recent CHF\n",
    " * hypertension\n",
    " * age > 75\n",
    " * Diabetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](./assets/chads2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[MELD: Model for End-Stage Renal Disease](https://www.ncbi.nlm.nih.gov/pubmed/11172350)\n",
    "Published: 2001\n",
    "\n",
    "Sample: ~260 patients\n",
    "\n",
    "`9.57 * log(creatinine) + 3.78 * log(total bilirubin) + 11.2 * log(INR) + 6.43`\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
